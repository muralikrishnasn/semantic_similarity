{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1596035153200",
      "display_name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)"
    },
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpeTuGgiRDIQ",
        "colab_type": "text"
      },
      "source": [
        "# Testing SentenceBERT for semantic similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VYQhfUTRDIV",
        "colab_type": "text"
      },
      "source": [
        "* https://medium.com/analytics-vidhya/semantic-similarity-in-sentences-and-bert-e8d34f5a4677\n",
        "* https://towardsdatascience.com/word-embedding-using-bert-in-python-dd5a86c00342\n",
        "* https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm4zSBNRRDIY",
        "colab_type": "text"
      },
      "source": [
        "Install hugginface transformers and sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "TOMXidlURDIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6286affe-168d-41a9-97b7-d66bd27bf975"
      },
      "source": [
        "!pip install transformers # https://github.com/huggingface/transformers\n",
        "!pip install -U sentence-transformers # https://github.com/UKPLab/sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=52a90466d86d9f837f62b8f511a86b32821a26caa011b44fbec45d0fe80ee2c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/e0/65ad8fd86eba720412d9ff102c4a3540a113bbc6cd29b01e7ecc33ebb1fa/sentence-transformers-0.3.2.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (0.8.1rc1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.2-cp36-none-any.whl size=93964 sha256=14827450456fd7d8d0f35964fa018c5c02c820c39ad05b9de3c6d5179c4690a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/06/a0/567f3651876165429f6510d3197b011652a25e547552816824\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "38SDBrm8RDIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33d511a1-2684-48c1-b6c4-f74a1c27872d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "model = SentenceTransformer('bert-large-nli-stsb-mean-tokens') # Load the BERT model. Semantic Textual Similarity models are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.24G/1.24G [01:12<00:00, 17.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rou4NPPZRDIz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the sts-benchmark data and remove lines that contain errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ZGReh5RDI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove \"warn_bad_lines=False\" to print the lines that have errors.\n",
        "train_df = pd.read_csv('sts-train.csv', sep='\\t', engine='python', header=None, encoding='utf-8', error_bad_lines=False, warn_bad_lines=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYj2k4K2RDI-",
        "colab_type": "text"
      },
      "source": [
        "## 2. A quick look at the dataset we are using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "q78HftpURDJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "53c30b11-53a0-4687-f7f6-a51f99bdc134"
      },
      "source": [
        "print(train_df.loc[0])\n",
        "print('\\n')\n",
        "print(train_df.loc[45])\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                  main-captions\n",
            "1                         MSRvid\n",
            "2                       2012test\n",
            "3                              1\n",
            "4                              5\n",
            "5         A plane is taking off.\n",
            "6    An air plane is taking off.\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\n",
            "0                     main-captions\n",
            "1                            MSRvid\n",
            "2                          2012test\n",
            "3                                68\n",
            "4                                 1\n",
            "5       A man is playing the piano.\n",
            "6    A woman is playing the violin.\n",
            "Name: 45, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>1</td>\n",
              "      <td>5.00</td>\n",
              "      <td>A plane is taking off.</td>\n",
              "      <td>An air plane is taking off.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>4</td>\n",
              "      <td>3.80</td>\n",
              "      <td>A man is playing a large flute.</td>\n",
              "      <td>A man is playing a flute.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>5</td>\n",
              "      <td>3.80</td>\n",
              "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
              "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>6</td>\n",
              "      <td>2.60</td>\n",
              "      <td>Three men are playing chess.</td>\n",
              "      <td>Two men are playing chess.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012test</td>\n",
              "      <td>9</td>\n",
              "      <td>4.25</td>\n",
              "      <td>A man is playing the cello.</td>\n",
              "      <td>A man seated is playing the cello.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               0  ...                                                  6\n",
              "0  main-captions  ...                        An air plane is taking off.\n",
              "1  main-captions  ...                          A man is playing a flute.\n",
              "2  main-captions  ...  A man is spreading shredded cheese on an uncoo...\n",
              "3  main-captions  ...                         Two men are playing chess.\n",
              "4  main-captions  ...                 A man seated is playing the cello.\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PISUb9NRDJK",
        "colab_type": "text"
      },
      "source": [
        "## 3. Comparing two sentence paires with SentenceBert as an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "NczbzQGURDJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f1f983b9-1ce5-499c-a9fb-3244d6a47791"
      },
      "source": [
        "s1 = train_df.loc[0][5]\n",
        "s2 = train_df.loc[0][6]\n",
        "s3 = train_df.loc[45][5]\n",
        "s4 = train_df.loc[45][6]\n",
        "\n",
        "print(f's1 = {s1}')\n",
        "print(f's2 = {s2}')\n",
        "print('\\n')\n",
        "print(f's3 = {s3}')\n",
        "print(f's4 = {s4}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s1 = A plane is taking off.\n",
            "s2 = An air plane is taking off.\n",
            "\n",
            "\n",
            "s3 = A man is playing the piano.\n",
            "s4 = A woman is playing the violin.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "wxexQas4RDJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "037300bf-a227-4fbc-c357-76ead4484c67"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "s1_embedding = model.encode(s1)\n",
        "s2_embedding = model.encode(s2)\n",
        "s3_embedding = model.encode(s3)\n",
        "s4_embedding = model.encode(s4)\n",
        "\n",
        "print(f's1 vs s2 = {distance.cosine(s1_embedding,s2_embedding)}')\n",
        "print(f'Human score = {train_df.loc[0][4]}')\n",
        "print(f'SentenceBERT Score = {round((1-distance.cosine(s1_embedding,s2_embedding))*5,1)}')\n",
        "\n",
        "print(f's3 vs s4 = {distance.cosine(s3_embedding,s4_embedding)}')\n",
        "print(f'Human score = {train_df.loc[45][4]}')\n",
        "print(f'SentenceBERT Score = {round((1-distance.cosine(s3_embedding,s4_embedding))*5,1)}')\n",
        "\n",
        "print(f's1 vs s3 = {distance.cosine(s1_embedding,s3_embedding)}')\n",
        "print(f's1 vs s4 = {distance.cosine(s1_embedding,s4_embedding)}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s1 vs s2 = 0.017929553985595703\n",
            "Human score = 5.0\n",
            "SentenceBERT Score = 4.9\n",
            "s3 vs s4 = 0.77469402551651\n",
            "Human score = 1.0\n",
            "SentenceBERT Score = 1.1\n",
            "s1 vs s3 = 0.8804589062929153\n",
            "s1 vs s4 = 0.8903428241610527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8u7aNcRDJe",
        "colab_type": "text"
      },
      "source": [
        "## 4. Getting the human scores and the SentenceBERT scores and comparing them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfAId4S3RDJf",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Load the data and preprocess it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSwIupk7RDJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "data = []\n",
        "with open('sts-dev.csv') as f:\n",
        "    for line in f.read().splitlines():\n",
        "        splits = line.split('\\t')\n",
        "        data.append({\n",
        "            'score': float(splits[4]),\n",
        "            's1': splits[5],\n",
        "            's2': splits[6]\n",
        "        })\n",
        "\n",
        "# removes punctuation from sentences\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "# lowercase, tokenize and remove punctuation from sentences\n",
        "for x in data:\n",
        "    x['s1'].lower()\n",
        "    x['s2'].lower()\n",
        "    x['s1'] = tokenizer.tokenize(x['s1'])\n",
        "    x['s2'] = tokenizer.tokenize(x['s2'])\n",
        "    x['s1'] = ' '.join(x['s1'])\n",
        "    x['s2'] = ' '.join(x['s2'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaRvcZ3ARDJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9f136164-8cc4-4de1-ebac-703b9850e7b6"
      },
      "source": [
        "data[3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'s1': 'A woman is playing the guitar',\n",
              " 's2': 'A man is playing guitar',\n",
              " 'score': 2.4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldezx2-qRDJu",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Get the scores and normalize them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k5WIvY_RDJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_human = []\n",
        "\n",
        "for x in data:\n",
        "    score = x['score']/5\n",
        "    score_human.append(score)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VZqHJv0RDJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_machine = []\n",
        "\n",
        "for x in data:\n",
        "    s1_embedding = model.encode(x['s1'])\n",
        "    s2_embedding = model.encode(x['s2'])\n",
        "    score = (1-distance.cosine(s1_embedding,s2_embedding))\n",
        "    score_machine.append(score)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuhKLlInRDJ8",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Compare human and fastText scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nno3vroRDJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2a51206-7504-450e-ec90-d59fd5395600"
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "result, _ = pearsonr(score_machine, score_human)\n",
        "print('Pearsonr:', end=' ')\n",
        "print(\"%.1f\" % (result*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearsonr: 87.8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}